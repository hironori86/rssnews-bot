"""
main.py

DX ãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åé›†ãƒ»é…ä¿¡ã‚¢ãƒ—ãƒªã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã€‚

CLI ã§å®Ÿè¡Œã™ã‚‹éš›ã¯ `--run-now` ã«ã‚ˆã‚‹å³æ™‚å®Ÿè¡Œã¨ã€æŒ‡å®šæ›œæ—¥ãƒ»æ™‚åˆ»ã«
è‡ªå‹•å®Ÿè¡Œã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¢ãƒ¼ãƒ‰ã‚’æä¾›ã™ã‚‹ã€‚ã¾ãŸã€`--post` ã‚ªãƒ—ã‚·ãƒ§ãƒ³
ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ç”Ÿæˆã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆæ¡ˆã‚„ note è¨˜äº‹ã‚’å®Ÿéš›ã«æŠ•ç¨¿ã™ã‚‹ã€‚
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import re
import sys
import time
from datetime import datetime, timedelta
from html import unescape
from pathlib import Path
from typing import Dict, List

import schedule

from config import CONFIG, Config
from utils import (
    categorizer, clustering, email_sender, line_sender,
    llm as llm_module, note, rss, teams, twitter
)


def setup_logging() -> None:
    """åŸºæœ¬çš„ãªãƒ­ã‚®ãƒ³ã‚°è¨­å®šã‚’è¡Œã†ã€‚"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    )


def strip_html_tags(text: str) -> str:
    """HTML ã‚¿ã‚°ã‚’ç°¡æ˜“çš„ã«é™¤å»ã—ã€ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚"""
    clean = re.sub(r"<[^>]+>", "", text)
    return unescape(clean)


def build_markdown(
    clusters: List[List[Dict[str, str]]], 
    summaries: Dict[str, str],
    categorized: Dict[str, List[Dict[str, str]]] | None = None
) -> str:
    """
    Teams ã¸é€ä¿¡ã™ã‚‹ Markdown ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çµ„ã¿ç«‹ã¦ã‚‹ã€‚

    Args:
        clusters: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆã€‚
        summaries: å„è¨˜äº‹ã®è¦ç´„ã€‚key ã¯è¨˜äº‹ã®ãƒªãƒ³ã‚¯ã€‚
        categorized: ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è¨˜äº‹è¾æ›¸

    Returns:
        Markdown å½¢å¼ã®æ–‡å­—åˆ—ã€‚
    """
    lines: List[str] = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    lines.append(f"## DXãƒ‹ãƒ¥ãƒ¼ã‚¹æ—¥æ¬¡ã¾ã¨ã‚ ({today})")
    lines.append(f"é›†è¨ˆæœŸé–“: {today}")
    lines.append(f"ç·è¨˜äº‹æ•°: {sum(len(cluster) for cluster in clusters)}ä»¶")
    
    if not clusters:
        lines.append("\næœ¬æœŸé–“å†…ã«è©²å½“ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return "\n".join(lines)
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è¡¨ç¤º
    if categorized:
        lines.append(categorizer.build_categorized_markdown(categorized, summaries))
    else:
        # å¾“æ¥ã®è¡¨ç¤º
        lines.append("\n### ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹")
        for cluster in clusters[:10]:
            article = cluster[0]
            title = article.get("title", "(ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜)")
            link = article.get("link", "")
            summary = summaries.get(link, "")
            lines.append(f"- [{title}]({link})")
            if summary:
                lines.append(f"  {summary}")
    
    return "\n".join(lines)


def select_topic_clusters(clusters: List[List[Dict[str, str]]], max_topics: int = 5) -> List[List[Dict[str, str]]]:
    """
    ãƒˆãƒ”ãƒƒã‚¯å€™è£œã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠã™ã‚‹ã€‚ã‚µã‚¤ã‚ºã®å¤§ãã„é †ã«ä¸¦ã¹æ›¿ãˆã€æœ€å¤§æ•°ã‚’è¿”ã™ã€‚

    Args:
        clusters: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆã€‚
        max_topics: æŠ½å‡ºã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯æ•°ã®ä¸Šé™ã€‚

    Returns:
        æŠ½å‡ºã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒªã‚¹ãƒˆã€‚
    """
    # ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºã¨å…¬é–‹æ—¥ã§ã‚½ãƒ¼ãƒˆï¼ˆã¾ãšã‚µã‚¤ã‚ºé™é †ã€æ¬¡ã«æœ€æ–°æ—¥ä»˜é™é †ï¼‰
    sorted_clusters = sorted(
        clusters,
        key=lambda cl: (
            -len(cl),
            max((art.get("published") for art in cl if art.get("published")), default=datetime.min),
        ),
    )
    return sorted_clusters[:max_topics]


def save_generated_content(
    tweet_messages: List[str],
    note_articles: List[Dict[str, str]],
    clusters: List[List[Dict[str, str]]],
    summaries: Dict[str, str],
    output_base_dir: str = "outputs",
    teams_markdown: str = "",
    categorized: Dict[str, List[Dict[str, str]]] | None = None,
    all_articles: List[Dict[str, str]] | None = None,
) -> str:
    """
    ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚

    Args:
        tweet_messages: ãƒ„ã‚¤ãƒ¼ãƒˆæ¡ˆã®ãƒªã‚¹ãƒˆ
        note_articles: noteè¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        clusters: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¨˜äº‹
        summaries: è¨˜äº‹ã®è¦ç´„
        all_articles: å…¨è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆé€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ä½œæˆç”¨ï¼‰

    Returns:
        ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    """
    # ç¾åœ¨ã®æ—¥æ™‚ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆæ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ç”¨ã®æ§‹é€ ï¼‰
    today = datetime.now()
    date_str = today.strftime("%Y%m%d")
    timestamp = today.strftime("%Y%m%d_%H%M%S")
    
    # æ—¥ä»˜ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
    daily_dir = Path(output_base_dir) / "daily" / date_str
    output_dir = daily_dir / timestamp
    tweets_dir = output_dir / "tweets"
    notes_dir = output_dir / "notes"
    teams_dir = output_dir / "teams"
    raw_data_dir = output_dir / "raw_data"
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    tweets_dir.mkdir(parents=True, exist_ok=True)
    notes_dir.mkdir(parents=True, exist_ok=True)
    teams_dir.mkdir(parents=True, exist_ok=True)
    raw_data_dir.mkdir(parents=True, exist_ok=True)
    
    logger = logging.getLogger("file_saver")
    
    # ãƒ„ã‚¤ãƒ¼ãƒˆä¿å­˜
    for i, tweet in enumerate(tweet_messages, 1):
        tweet_file = tweets_dir / f"tweet_{i:02d}.txt"
        tweet_file.write_text(tweet, encoding="utf-8")
    
    # noteè¨˜äº‹ä¿å­˜ï¼ˆnoteã«ã‚³ãƒ”ãƒšã—ã‚„ã™ã„å½¢å¼ï¼‰
    for i, article in enumerate(note_articles, 1):
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚ˆã‚Šåˆ†ã‹ã‚Šã‚„ã™ã
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', article['title'][:50])
        note_file = notes_dir / f"note_{i:02d}_{safe_title}.md"
        
        # noteã«ãã®ã¾ã¾ã‚³ãƒ”ãƒšã§ãã‚‹å½¢å¼
        content = f"""# {article['title']}

{article['content']}

---
ğŸ”— ã“ã®è¨˜äº‹ã‚’noteã«ã‚³ãƒ”ãƒšã™ã‚‹éš›ã¯ã€ä¸Šè¨˜ã®å†…å®¹ã‚’ãã®ã¾ã¾è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚
ğŸ“ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€noteã§ã‚‚é©åˆ‡ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
"""
        note_file.write_text(content, encoding="utf-8")
    
    # Teamsé€šçŸ¥å†…å®¹ã‚’ä¿å­˜
    if teams_markdown:
        teams_file = teams_dir / "teams_notification.md"
        teams_file.write_text(teams_markdown, encoding="utf-8")
    
    # é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ä½œæˆç”¨ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    if all_articles:
        raw_articles_data = {
            "date": date_str,
            "generated_at": today.isoformat(),
            "articles": [
                {
                    "title": article.get("title", ""),
                    "link": article.get("link", ""),
                    "published": article.get("published", "").isoformat() if isinstance(article.get("published"), datetime) else str(article.get("published", "")),
                    "source": article.get("source", ""),
                    "summary": summaries.get(article.get("link", ""), ""),
                    "category": None  # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å¾Œã§è¿½åŠ 
                }
                for article in all_articles
            ]
        }
        
        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’è¿½åŠ 
        if categorized:
            for category, category_articles in categorized.items():
                for category_article in category_articles:
                    article_link = category_article.get("link", "")
                    for raw_article in raw_articles_data["articles"]:
                        if raw_article["link"] == article_link:
                            raw_article["category"] = category
                            break
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        raw_data_file = raw_data_dir / "articles.json"
        raw_data_file.write_text(
            json.dumps(raw_articles_data, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        
        # æ—¥ä»˜åˆ¥ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆï¼ˆé€±é–“é›†è¨ˆç”¨ï¼‰
        daily_index_file = daily_dir / "index.json"
        daily_index = {
            "date": date_str,
            "processed_at": today.isoformat(),
            "data_path": raw_data_file.relative_to(daily_dir).as_posix(),
            "total_articles": len(all_articles),
            "categories": {
                category: len(articles)
                for category, articles in (categorized.items() if categorized else {})
            }
        }
        daily_index_file.write_text(
            json.dumps(daily_index, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’JSONã§ä¿å­˜
    summary_data = {
        "type": "daily",
        "date": date_str,
        "generated_at": today.isoformat(),
        "total_articles": sum(len(cluster) for cluster in clusters),
        "total_clusters": len(clusters),
        "tweets_count": len(tweet_messages),
        "notes_count": len(note_articles),
        "tweets": [
            {
                "index": i,
                "content": tweet,
                "length": len(tweet)
            }
            for i, tweet in enumerate(tweet_messages, 1)
        ],
        "notes": [
            {
                "index": i,
                "title": article["title"],
                "length": len(article["content"])
            }
            for i, article in enumerate(note_articles, 1)
        ],
        "clusters": [
            {
                "cluster_index": i,
                "articles_count": len(cluster),
                "main_article": {
                    "title": cluster[0].get("title", ""),
                    "link": cluster[0].get("link", ""),
                    "summary": summaries.get(cluster[0].get("link", ""), "")
                }
            }
            for i, cluster in enumerate(clusters[:5], 1)
        ],
        "categorized": {
            category: len(articles)
            for category, articles in (categorized.items() if categorized else {})
        }
    }
    
    summary_file = output_dir / "summary.json"
    summary_file.write_text(
        json.dumps(summary_data, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    
    # ä¿å­˜çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä»¥ä¸‹ã«ä¿å­˜ã—ã¾ã—ãŸ:")
    logger.info(f"  ğŸ“¦ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    logger.info(f"  ğŸ”Š XæŠ•ç¨¿æ¡ˆ: {len(tweet_messages)}ä»¶")
    logger.info(f"  ğŸ“ noteè¨˜äº‹æ¡ˆ: {len(note_articles)}ä»¶")
    logger.info(f"  ğŸ“‹ ã‚µãƒãƒªãƒ¼: {summary_file.name}")
    
    return str(output_dir)


def run_task(config: Config, post: bool) -> None:
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‹ã‚‰è¦ç´„ã€æŠ•ç¨¿ç”Ÿæˆã€Teams ã¸ã®é€šçŸ¥ãŠã‚ˆã³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æŠ•ç¨¿ã‚’å®Ÿè¡Œã™ã‚‹ä¸»å‡¦ç†é–¢æ•°ã€‚

    Args:
        config: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã€‚
        post: X ã¨ note ã¸ã®å®ŸæŠ•ç¨¿ã‚’è¡Œã†ã‹ã©ã†ã‹ã€‚
    """
    logger = logging.getLogger("runner")
    logger.info("ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    try:
        articles = rss.fetch_articles(config.rss_feeds, days=1, keywords=config.keywords)
    except Exception as e:
        logger.error("RSS ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)
        return
    if not articles:
        # è¨˜äº‹ãŒãªã„å ´åˆã§ã‚‚ Teams ã¸é€šçŸ¥ã™ã‚‹
        markdown = build_markdown([], {})
        try:
            teams.send_to_teams(config.team_webhook_url, markdown)
        except Exception:
            logger.exception("Teams ã¸ã®é€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # é‡è¤‡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    clusters = clustering.cluster_articles(articles)
    logger.info("ã‚¯ãƒ©ã‚¹ã‚¿æ•°: %d", len(clusters))

    # LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    llm_client = llm_module.LLMClient(api_key=config.openai_api_key)

    # å„è¨˜äº‹è¦ç´„ã‚’ç”Ÿæˆ
    summaries: Dict[str, str] = {}
    for article in articles:
        link = article.get("link", "")
        text = article.get("summary") or article.get("title") or ""
        clean_text = strip_html_tags(text)
        try:
            summary = llm_client.summarize(clean_text, max_chars=200)
        except Exception as e:
            logger.error("è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ (%s): %s", link, e)
            summary = clean_text[:200]
        summaries[link] = summary

    # è¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
    categorized = categorizer.categorize_articles(articles, summaries)
    
    # Teams ç”¨ Markdown ã‚’ä½œæˆã—ã¦é€ä¿¡
    markdown = build_markdown(clusters, summaries, categorized)
    try:
        teams.send_to_teams(config.team_webhook_url, markdown)
    except Exception:
        logger.exception("Teams ã¸ã®é€šçŸ¥ã«å¤±æ•—ã—ã¾ã—ãŸ")

    # ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    topic_clusters = select_topic_clusters(clusters, max_topics=3)
    logger.info("ãƒˆãƒ”ãƒƒã‚¯æ•°: %d", len(topic_clusters))
    
    # æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ç”¨ã®æ—¥ä»˜è¨­å®š
    today = datetime.now()
    today_str = today.strftime("%Y/%m/%d")
    
    # æ³¨ç›®ãƒˆãƒ”ãƒƒã‚¯ã®æƒ…å ±ã‚’å–å¾—
    top_topics = []
    for cl in topic_clusters[:3]:
        canonical = cl[0]
        title = canonical.get("title", "")
        link = canonical.get("link", "")
        summary = summaries.get(link, "")
        top_topics.append({"title": title, "link": link, "summary": summary})
    
    # æ—¥æ¬¡ãƒ„ã‚¤ãƒ¼ãƒˆæ¡ˆç”Ÿæˆ
    tweet_messages: List[str] = []
    try:
        daily_tweet = llm_client.generate_daily_tweet(
            articles, 
            [t["title"] for t in top_topics],
            today_str
        )
        tweet_messages.append(daily_tweet)
    except Exception as e:
        logger.error("æ—¥æ¬¡ãƒ„ã‚¤ãƒ¼ãƒˆæ¡ˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)
        fallback_tweet = (
            f"ğŸ“Šä»Šæ—¥ã®DXãƒ‹ãƒ¥ãƒ¼ã‚¹({today_str})\n\n"
            f"ç·è¨˜äº‹æ•°: {len(articles)}ä»¶\n"
            f"æ³¨ç›®: {top_topics[0]['title'] if top_topics else ''}ãªã©\n\n"
            f"è©³ç´°ã¯ãƒªãƒ³ã‚¯ã‹ã‚‰ğŸ‘‡\n"
            f"#DXæ—¥å ± #AIãƒ‹ãƒ¥ãƒ¼ã‚¹"
        )
        tweet_messages.append(fallback_tweet[:280])
    
    # æ—¥æ¬¡noteè¨˜äº‹æ¡ˆç”Ÿæˆ
    note_articles: List[Dict[str, str]] = []
    try:
        daily_note = llm_client.generate_daily_note_article(
            articles,
            top_topics,
            today_str,
            categorized,
            summaries
        )
        note_articles.append({
            "title": f"ä»Šæ—¥ã®DXãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚({today_str})",
            "content": daily_note
        })
    except Exception as e:
        logger.error("æ—¥æ¬¡noteè¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)
        fallback_note = (
            f"ä»Šæ—¥({today_str})ã®DXãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚\n\n"
            f"ç·è¨˜äº‹æ•°: {len(articles)}ä»¶\n\n"
            f"æ³¨ç›®ãƒˆãƒ”ãƒƒã‚¯:\n"
        )
        for i, topic in enumerate(top_topics[:3], 1):
            fallback_note += f"{i}. {topic['title']}\n{topic['summary']}\n\n"
        note_articles.append({
            "title": f"ä»Šæ—¥ã®DXãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚({today_str})",
            "content": fallback_note
        })

    # ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_dir = save_generated_content(
        tweet_messages, note_articles, clusters, summaries, 
        config.output_dir, markdown, categorized, articles
    )
    logger.info(f"ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_dir}")

    # ãƒ¡ãƒ¼ãƒ«é€ä¿¡
    if config.smtp_server and config.email_recipients:
        try:
            email_client = email_sender.EmailSender(
                config.smtp_server,
                config.smtp_port,
                config.smtp_username or "",
                config.smtp_password or "",
                config.from_email or ""
            )
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨˜äº‹æ•°ã‚’å–å¾—
            category_counts = {
                category: len(articles)
                for category, articles in categorized.items()
            }
            
            html_content = email_sender.format_weekly_report_html(
                today_str, today_str, len(articles),
                top_topics, category_counts
            )
            text_content = email_sender.format_weekly_report_text(
                today_str, today_str, len(articles),
                top_topics, category_counts
            )
            
            subject = f"DXãƒ‹ãƒ¥ãƒ¼ã‚¹æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆ ({today_str})"
            email_client.send_weekly_report(
                config.email_recipients,
                subject,
                html_content,
                text_content
            )
        except Exception:
            logger.exception("ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    # LINEé€ä¿¡
    if config.line_channel_access_token:
        try:
            line_client = line_sender.LineSender(config.line_channel_access_token)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨˜äº‹æ•°ã‚’å–å¾—
            category_counts = {
                category: len(articles)
                for category, articles in categorized.items()
            }
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å€‹åˆ¥é€ä¿¡ã€ãã†ã§ãªã‘ã‚Œã°ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
            if config.line_user_ids:
                for user_id in config.line_user_ids:
                    line_client.send_weekly_report(
                        today_str, today_str,
                        len(articles), top_topics, 
                        category_counts, markdown, user_id
                    )
            else:
                line_client.send_weekly_report(
                    today_str, today_str,
                    len(articles), top_topics,
                    category_counts, markdown
                )
        except Exception:
            logger.exception("LINEé€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    # å®ŸæŠ•ç¨¿
    if post:
        logger.info("post ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæœ‰åŠ¹ãªãŸã‚ã€X/note ã¸æŠ•ç¨¿ã—ã¾ã™ã€‚")
        for tweet_text in tweet_messages:
            try:
                twitter.post_to_twitter(config.twitter_bearer_token, tweet_text)
            except Exception:
                logger.exception("Twitter ã¸ã®æŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        for article in note_articles:
            try:
                note.post_to_note(config.note_token, article["title"], article["content"])
            except Exception:
                logger.exception("note ã¸ã®æŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ")


def schedule_tasks(config: Config, post: bool) -> None:
    """
    schedule ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’åˆ©ç”¨ã—ã¦æŒ‡å®šæ›œæ—¥ãƒ»æ™‚åˆ»ã«ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

    Args:
        config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        post: post ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‚
    """
    logger = logging.getLogger("scheduler")

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«é–¢æ•°ã«ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£ã‚’æ¸¡ã™
    def job_wrapper() -> None:
        run_task(config, post)

    # æ›œæ—¥æ–‡å­—åˆ—ã‹ã‚‰ schedule ã®ãƒ¡ã‚½ãƒƒãƒ‰åã«ãƒãƒƒãƒ”ãƒ³ã‚°
    day_method_map = {
        "mon": schedule.every().monday,
        "tue": schedule.every().tuesday,
        "wed": schedule.every().wednesday,
        "thu": schedule.every().thursday,
        "fri": schedule.every().friday,
        "sat": schedule.every().saturday,
        "sun": schedule.every().sunday,
    }
    method = day_method_map.get(config.post_day_of_week.lower())
    if not method:
        logger.error("ç„¡åŠ¹ãª POST_DAY_OF_WEEK: %s", config.post_day_of_week)
        sys.exit(1)
    time_str = f"{config.post_hour_24}:00"
    logger.info("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’è¨­å®šã—ã¾ã™: %s %s", config.post_day_of_week, time_str)
    method.at(time_str).do(job_wrapper)
    while True:
        schedule.run_pending()
        time.sleep(60)


def main() -> None:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã—ã€å‡¦ç†ã‚’é–‹å§‹ã™ã‚‹ã€‚"""
    setup_logging()
    parser = argparse.ArgumentParser(description="DX ãƒ‹ãƒ¥ãƒ¼ã‚¹è‡ªå‹•åé›†ãƒ»é…ä¿¡ã‚¢ãƒ—ãƒª")
    parser.add_argument("--run-now", action="store_true", help="ç›´ã¡ã«å®Ÿè¡Œã™ã‚‹ (ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç„¡è¦–)")
    parser.add_argument("--post", action="store_true", help="ç”Ÿæˆã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆãŠã‚ˆã³ note è¨˜äº‹ã‚’å®Ÿéš›ã«æŠ•ç¨¿ã™ã‚‹")
    args = parser.parse_args()

    config = CONFIG
    if not config:
        logging.error("è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¿…é ˆç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    if args.run_now:
        run_task(config, args.post)
    else:
        schedule_tasks(config, args.post)


if __name__ == "__main__":
    main()
