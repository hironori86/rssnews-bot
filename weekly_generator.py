"""
weekly_generator.py

æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

éå»7æ—¥åˆ†ã®æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€çµ±åˆãƒ»å†ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã¦
é€±æ¬¡ãƒ„ã‚¤ãƒ¼ãƒˆãƒ»noteè¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹ã€‚
"""

from __future__ import annotations

import argparse
import json
import logging
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List

from config import CONFIG, Config
from utils import categorizer, clustering, llm as llm_module


def setup_logging() -> None:
    """åŸºæœ¬çš„ãªãƒ­ã‚®ãƒ³ã‚°è¨­å®šã‚’è¡Œã†ã€‚"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    )


def load_daily_data(base_dir: str, days: int = 7) -> List[Dict]:
    """
    éå»Næ—¥åˆ†ã®æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ã€‚

    Args:
        base_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        days: èª­ã¿è¾¼ã‚€æ—¥æ•°

    Returns:
        æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    logger = logging.getLogger("data_loader")
    daily_data = []
    base_path = Path(base_dir) / "daily"
    
    # éå»Næ—¥åˆ†ã®æ—¥ä»˜ã‚’ç”Ÿæˆ
    today = datetime.now()
    for i in range(days):
        target_date = today - timedelta(days=i)
        date_str = target_date.strftime("%Y%m%d")
        daily_dir = base_path / date_str
        
        if not daily_dir.exists():
            logger.warning(f"æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {date_str}")
            continue
            
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        index_file = daily_dir / "index.json"
        if not index_file.exists():
            logger.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {date_str}")
            continue
            
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            data_path = daily_dir / index_data["data_path"]
            if not data_path.exists():
                logger.warning(f"è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
                continue
                
            with open(data_path, 'r', encoding='utf-8') as f:
                articles_data = json.load(f)
            
            daily_data.append({
                "date": date_str,
                "index": index_data,
                "articles": articles_data
            })
            logger.info(f"æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {date_str} ({len(articles_data['articles'])}ä»¶)")
            
        except Exception as e:
            logger.error(f"æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ ({date_str}): {e}")
            continue
    
    return daily_data


def merge_articles(daily_data: List[Dict]) -> tuple[List[Dict[str, str]], Dict[str, str]]:
    """
    è¤‡æ•°æ—¥åˆ†ã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã€é‡è¤‡ã‚’é™¤å»ã™ã‚‹ã€‚

    Args:
        daily_data: æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

    Returns:
        çµ±åˆã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆã¨è¦ç´„è¾æ›¸ã®ã‚¿ãƒ—ãƒ«
    """
    logger = logging.getLogger("article_merger")
    
    merged_articles = []
    summaries = {}
    seen_links = set()
    
    for day_data in daily_data:
        articles_data = day_data["articles"]
        
        for article in articles_data["articles"]:
            link = article.get("link", "")
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if link in seen_links:
                continue
                
            seen_links.add(link)
            
            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
            merged_article = {
                "title": article.get("title", ""),
                "link": link,
                "published": article.get("published", ""),
                "source": article.get("source", ""),
                "summary": article.get("summary", ""),
                "category": article.get("category", "ãã®ä»–")
            }
            
            merged_articles.append(merged_article)
            summaries[link] = article.get("summary", "")
    
    logger.info(f"è¨˜äº‹ã‚’çµ±åˆã—ã¾ã—ãŸ: {len(merged_articles)}ä»¶ (é‡è¤‡é™¤å»å‰: {sum(len(d['articles']['articles']) for d in daily_data)}ä»¶)")
    return merged_articles, summaries


def categorize_merged_articles(articles: List[Dict[str, str]]) -> Dict[str, List[Dict[str, str]]]:
    """
    çµ±åˆã•ã‚ŒãŸè¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡ã™ã‚‹ã€‚

    Args:
        articles: çµ±åˆã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆ

    Returns:
        ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨˜äº‹è¾æ›¸
    """
    categorized = {
        "æœ€æ–°æŠ€è¡“å‹•å‘": [],
        "å°å…¥ãƒ»æ´»ç”¨äº‹ä¾‹": [],
        "ãã®ä»–": []
    }
    
    for article in articles:
        category = article.get("category", "ãã®ä»–")
        if category in categorized:
            categorized[category].append(article)
        else:
            categorized["ãã®ä»–"].append(article)
    
    return categorized


def generate_weekly_content(
    articles: List[Dict[str, str]], 
    summaries: Dict[str, str],
    categorized: Dict[str, List[Dict[str, str]]],
    config: Config,
    start_date: str,
    end_date: str
) -> tuple[List[str], List[Dict[str, str]]]:
    """
    é€±é–“ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆãƒ»noteè¨˜äº‹ï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚

    Args:
        articles: çµ±åˆã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆ
        summaries: è¨˜äº‹ã®è¦ç´„è¾æ›¸
        categorized: ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨˜äº‹
        config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        start_date: é–‹å§‹æ—¥
        end_date: çµ‚äº†æ—¥

    Returns:
        ãƒ„ã‚¤ãƒ¼ãƒˆãƒªã‚¹ãƒˆã¨noteè¨˜äº‹ãƒªã‚¹ãƒˆã®ã‚¿ãƒ—ãƒ«
    """
    logger = logging.getLogger("content_generator")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    clusters = clustering.cluster_articles(articles)
    logger.info(f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(clusters)}")
    
    # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    llm_client = llm_module.LLMClient(api_key=config.openai_api_key)
    
    # ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºï¼ˆä¸Šä½3ã¤ï¼‰
    topic_clusters = sorted(
        clusters,
        key=lambda cl: (
            -len(cl),
            max((art.get("published") for art in cl if art.get("published")), default=""),
        ),
    )[:3]
    
    # æ³¨ç›®ãƒˆãƒ”ãƒƒã‚¯ã®æƒ…å ±ã‚’å–å¾—
    top_topics = []
    for cl in topic_clusters:
        canonical = cl[0]
        title = canonical.get("title", "")
        link = canonical.get("link", "")
        summary = summaries.get(link, "")
        top_topics.append({"title": title, "link": link, "summary": summary})
    
    # é€±æ¬¡ãƒ„ã‚¤ãƒ¼ãƒˆæ¡ˆç”Ÿæˆ
    tweet_messages = []
    try:
        weekly_tweet = llm_client.generate_weekly_tweet(
            articles, 
            [t["title"] for t in top_topics],
            start_date,
            end_date
        )
        tweet_messages.append(weekly_tweet)
    except Exception as e:
        logger.error(f"é€±æ¬¡ãƒ„ã‚¤ãƒ¼ãƒˆæ¡ˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        fallback_tweet = (
            f"ğŸ“Šä»Šé€±ã®DXãƒ‹ãƒ¥ãƒ¼ã‚¹({start_date}ã€œ{end_date})\n\n"
            f"ç·è¨˜äº‹æ•°: {len(articles)}ä»¶\n"
            f"æ³¨ç›®: {top_topics[0]['title'] if top_topics else ''}ãªã©\n\n"
            f"è©³ç´°ã¯ãƒªãƒ³ã‚¯ã‹ã‚‰ğŸ‘‡\n"
            f"#DXé€±å ± #AIãƒ‹ãƒ¥ãƒ¼ã‚¹"
        )
        tweet_messages.append(fallback_tweet[:280])
    
    # é€±æ¬¡noteè¨˜äº‹æ¡ˆç”Ÿæˆ
    note_articles = []
    try:
        weekly_note = llm_client.generate_weekly_note_article(
            articles,
            top_topics,
            start_date,
            end_date,
            categorized,
            summaries
        )
        note_articles.append({
            "title": f"ä»Šé€±ã®DXãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚({start_date}ã€œ{end_date})",
            "content": weekly_note
        })
    except Exception as e:
        logger.error(f"é€±æ¬¡noteè¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        fallback_note = (
            f"ä»Šé€±({start_date}ã€œ{end_date})ã®DXãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚\n\n"
            f"ç·è¨˜äº‹æ•°: {len(articles)}ä»¶\n\n"
            f"æ³¨ç›®ãƒˆãƒ”ãƒƒã‚¯:\n"
        )
        for i, topic in enumerate(top_topics[:3], 1):
            fallback_note += f"{i}. {topic['title']}\n{topic['summary']}\n\n"
        note_articles.append({
            "title": f"ä»Šé€±ã®DXãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚({start_date}ã€œ{end_date})",
            "content": fallback_note
        })
    
    return tweet_messages, note_articles


def save_weekly_content(
    tweet_messages: List[str],
    note_articles: List[Dict[str, str]],
    articles: List[Dict[str, str]],
    summaries: Dict[str, str],
    categorized: Dict[str, List[Dict[str, str]]],
    output_base_dir: str,
    start_date: str,
    end_date: str
) -> str:
    """
    é€±é–“ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚

    Args:
        tweet_messages: ãƒ„ã‚¤ãƒ¼ãƒˆæ¡ˆã®ãƒªã‚¹ãƒˆ
        note_articles: noteè¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        articles: çµ±åˆã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆ
        summaries: è¨˜äº‹ã®è¦ç´„è¾æ›¸
        categorized: ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨˜äº‹
        output_base_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        start_date: é–‹å§‹æ—¥
        end_date: çµ‚äº†æ—¥

    Returns:
        ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    """
    # é€±é–“ãƒ‡ãƒ¼ã‚¿ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    weekly_dir = Path(output_base_dir) / "weekly" / f"{start_date}_{end_date}"
    output_dir = weekly_dir / timestamp
    tweets_dir = output_dir / "tweets"
    notes_dir = output_dir / "notes"
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    tweets_dir.mkdir(parents=True, exist_ok=True)
    notes_dir.mkdir(parents=True, exist_ok=True)
    
    logger = logging.getLogger("file_saver")
    
    # ãƒ„ã‚¤ãƒ¼ãƒˆä¿å­˜
    for i, tweet in enumerate(tweet_messages, 1):
        tweet_file = tweets_dir / f"tweet_{i:02d}.txt"
        tweet_file.write_text(tweet, encoding="utf-8")
    
    # noteè¨˜äº‹ä¿å­˜ï¼ˆnoteã«ã‚³ãƒ”ãƒšã—ã‚„ã™ã„å½¢å¼ï¼‰
    for i, article in enumerate(note_articles, 1):
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚ˆã‚Šåˆ†ã‹ã‚Šã‚„ã™ã
        import re
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', article['title'][:50])
        note_file = notes_dir / f"note_{i:02d}_{safe_title}.md"
        
        # noteã«ãã®ã¾ã¾ã‚³ãƒ”ãƒšã§ãã‚‹å½¢å¼
        content = f"""# {article['title']}

{article['content']}

---
ğŸ”— ã“ã®è¨˜äº‹ã‚’noteã«ã‚³ãƒ”ãƒšã™ã‚‹éš›ã¯ã€ä¸Šè¨˜ã®å†…å®¹ã‚’ãã®ã¾ã¾è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚
ğŸ“ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€noteã§ã‚‚é©åˆ‡ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
"""
        note_file.write_text(content, encoding="utf-8")
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’JSONã§ä¿å­˜
    summary_data = {
        "type": "weekly",
        "start_date": start_date,
        "end_date": end_date,
        "generated_at": datetime.now().isoformat(),
        "total_articles": len(articles),
        "tweets_count": len(tweet_messages),
        "notes_count": len(note_articles),
        "categorized": {
            category: len(category_articles)
            for category, category_articles in categorized.items()
        }
    }
    
    summary_file = output_dir / "summary.json"
    summary_file.write_text(
        json.dumps(summary_data, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    
    # ä¿å­˜çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"ğŸ“ é€±é–“ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä»¥ä¸‹ã«ä¿å­˜ã—ã¾ã—ãŸ:")
    logger.info(f"  ğŸ“¦ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    logger.info(f"  ğŸ”Š XæŠ•ç¨¿æ¡ˆ: {len(tweet_messages)}ä»¶")
    logger.info(f"  ğŸ“ noteè¨˜äº‹æ¡ˆ: {len(note_articles)}ä»¶")
    logger.info(f"  ğŸ“‹ ã‚µãƒãƒªãƒ¼: {summary_file.name}")
    
    return str(output_dir)


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚"""
    setup_logging()
    
    parser = argparse.ArgumentParser(description="æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç”Ÿæˆ")
    parser.add_argument("--days", type=int, default=7, help="å¯¾è±¡ã¨ã™ã‚‹æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 7ï¼‰")
    parser.add_argument("--output-dir", default="outputs", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: outputsï¼‰")
    args = parser.parse_args()
    
    config = CONFIG
    if not config:
        logging.error("è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¿…é ˆç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    
    logger = logging.getLogger("weekly_generator")
    logger.info("é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    
    # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    daily_data = load_daily_data(args.output_dir, args.days)
    if not daily_data:
        logger.error("æœ‰åŠ¹ãªæ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(1)
    
    # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
    merged_articles, summaries = merge_articles(daily_data)
    if not merged_articles:
        logger.error("çµ±åˆã™ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(1)
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†é¡
    categorized = categorize_merged_articles(merged_articles)
    
    # æ—¥ä»˜ç¯„å›²ã®è¨ˆç®—
    end_date = datetime.now()
    start_date = end_date - timedelta(days=args.days-1)
    start_date_str = start_date.strftime("%Y/%m/%d")
    end_date_str = end_date.strftime("%Y/%m/%d")
    
    # é€±é–“ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
    tweet_messages, note_articles = generate_weekly_content(
        merged_articles, summaries, categorized, config, 
        start_date_str, end_date_str
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    output_dir = save_weekly_content(
        tweet_messages, note_articles, merged_articles, summaries, categorized,
        args.output_dir, start_date.strftime("%Y%m%d"), end_date.strftime("%Y%m%d")
    )
    
    logger.info(f"é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ: {output_dir}")


if __name__ == "__main__":
    main()